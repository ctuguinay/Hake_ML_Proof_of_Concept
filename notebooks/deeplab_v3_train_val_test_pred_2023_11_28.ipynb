{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import xarray as xr\n",
    "from torch.utils.ds_MVBSimport random_split\n",
    "import pytorch_lightning as pl\n",
    "from typing import Dict, List, Union\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision.models.segmentation.deeplabv3 import ASPP\n",
    "from torchvision.models.resnet import resnet50\n",
    "from torchvision.models._utils import IntermediateLayerGetter\n",
    "from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HakeXarrayDatasets(Dataset):\n",
    "    def __init__(self,\n",
    "                 data: Union[List[str],Dict[str, str]],\n",
    "                 desired_order: List[str] = [\"120 kHz\", \"38 kHz\", \"18 kHz\"],\n",
    "                 time_span: str=\"10min\",\n",
    "                 max_depth: float=800):\n",
    "        self.desired_order = desired_order\n",
    "        self.time_span = time_span\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "        if isinstance(data, list):\n",
    "            self.is_predict = True\n",
    "            self.file_dict = {}\n",
    "            self.load_from_data_dir_list(data)\n",
    "        elif isinstance(data, dict):\n",
    "            self.is_predict = False\n",
    "            self.file_dict = {}\n",
    "            self.load_from_target_data_dict(data)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid input type for 'data'. It should be either a list or a dictionary.\")\n",
    "\n",
    "    def load_from_target_data_dict(self, target_data_dir_dict):\n",
    "        counter = 0\n",
    "        for mask_dir, ds_MVBS_dir in target_data_dir_dict.items():\n",
    "            ds_MVBS_dir = Path(ds_MVBS_dir)\n",
    "            mask_dir = Path(mask_dir)\n",
    "\n",
    "            # Get all .zarr files in the ds_MVBS directory\n",
    "            ds_MVBS_files = list(ds_MVBS_dir.glob('*.zarr'))\n",
    "\n",
    "            for ds_MVBS_file in tqdm(ds_MVBS_files[:30]):\n",
    "                # Extract filename without considering the path\n",
    "                ds_MVBS_filename = ds_MVBS_file.name\n",
    "\n",
    "                # Find a matching target file by filename in the target directory\n",
    "                mask_file = self.find_matching_target(ds_MVBS_filename, mask_dir)\n",
    "\n",
    "                if mask_file is not None:\n",
    "                    if self.check_ds_MVBS_file(ds_MVBS_file):\n",
    "                        ds_MVBS = xr.open_dataset(ds_MVBS_file)\n",
    "                        resampled_obj = ds_MVBS[\"ping_time\"].resample(\n",
    "                            ping_time=self.time_span, skipna=True\n",
    "                        )\n",
    "                        for _,ds_MVBS_ping_time_slice in resampled_obj.groups.items():\n",
    "                            self.file_dict[counter] = {\"ds_MVBS_file\": ds_MVBS_file,\n",
    "                                                    \"mask_file\": mask_file,\n",
    "                                                    \"ping_time_slice\": ds_MVBS_ping_time_slice}\n",
    "                            counter += 1\n",
    "\n",
    "    def load_from_data_dir_list(self, ds_MVBS_dirs):\n",
    "        counter = 0\n",
    "        for ds_MVBS_dir in ds_MVBS_dirs:\n",
    "            ds_MVBS_dir = Path(ds_MVBS_dir)\n",
    "\n",
    "            # Get all .zarr files in the ds_MVBSdirectory\n",
    "            unchecked_ds_MVBS_files = list(ds_MVBS_dir.glob('*.zarr'))\n",
    "\n",
    "            for ds_MVBS_file in tqdm(unchecked_ds_MVBS_files[:30]):\n",
    "                if self.check_ds_MVBS_file(ds_MVBS_file):\n",
    "                    ds_MVBS = xr.open_dataset(ds_MVBS_file)\n",
    "                    resampled_obj = ds_MVBS[\"ping_time\"].resample(\n",
    "                        ping_time=self.time_span, skipna=True\n",
    "                    )\n",
    "                    for _, ping_time_slice in resampled_obj.groups.items():\n",
    "                        self.file_dict[counter] = {\"ds_MVBS_file\": ds_MVBS_file,\n",
    "                                                   \"ping_time_slice\": ping_time_slice}\n",
    "                        counter += 1\n",
    "\n",
    "    def check_ds_MVBS_file(self, ds_MVBS_file):\n",
    "        try:\n",
    "            # Open datasets and check for variables and desired channels\n",
    "            data_ds = xr.open_dataset(ds_MVBS_file)\n",
    "            data_channels = data_ds[\"Sv\"].channel.values\n",
    "\n",
    "            return all(any(partial_name in ch for ch in data_channels) for partial_name in self.desired_order)\n",
    "\n",
    "        except KeyError as e:\n",
    "            # Print KeyError and file paths\n",
    "            print(f\"KeyError: {e}, File Paths: {ds_MVBS_file}\")\n",
    "            return False\n",
    "\n",
    "    def find_matching_target(self, ds_MVBS_filename, mask_dir):\n",
    "        mask_files = list(mask_dir.glob('*.zarr'))\n",
    "\n",
    "        for mask_file in mask_files:\n",
    "            if mask_file.name == ds_MVBS_filename:\n",
    "                return mask_file\n",
    "\n",
    "        return None\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.file_dict:\n",
    "            return len(self.file_dict)\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.file_dict is not None and self.is_predict:\n",
    "            file_dict_idx = self.file_dict[idx]\n",
    "            ds_MVBS_partition = xr.open_dataset(\n",
    "                file_dict_idx[\"ds_MVBS_file\"]\n",
    "            ).isel(ping_time=file_dict_idx[\"ping_time_slice\"]\n",
    "            ).sel(depth=slice(0, self.max_depth))\n",
    "\n",
    "            return ds_MVBS_partition\n",
    "\n",
    "        elif self.file_dict is not None and not self.is_predict:\n",
    "            file_dict_idx = self.file_dict[idx]\n",
    "            ds_MVBS_partition = xr.open_dataset(\n",
    "                file_dict_idx[\"ds_MVBS_file\"]\n",
    "            ).isel(ping_time=file_dict_idx[\"ping_time_slice\"]\n",
    "            ).sel(depth=slice(0, self.max_depth))\n",
    "\n",
    "            # Mask Processing\n",
    "            mask_partition = xr.open_dataset(\n",
    "                file_dict_idx[\"mask_file\"]\n",
    "            )[\"mask\"].isel(ping_time=file_dict_idx[\"ping_time_slice\"]\n",
    "            ).sel(depth=slice(0, self.max_depth)\n",
    "            ).transpose(\"depth\", \"ping_time\")\n",
    "\n",
    "            return ds_MVBS_partition, mask_partition\n",
    "        else:\n",
    "            raise IndexError(\"Index out of range. No ds_MVBSfiles available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 184.52it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 305.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Length: 356\n",
      "Coordinates:\n",
      "  * channel    (channel) <U35 'GPT  18 kHz 009072034d55 3 ES18-11' ... 'GPT 2...\n",
      "  * depth      (depth) float64 0.0 0.2 0.4 0.6 0.8 ... 399.4 399.6 399.8 400.0\n",
      "  * ping_time  (ping_time) datetime64[ns] 2007-08-17T17:08:30 ... 2007-08-17T...\n",
      "Coordinates:\n",
      "  * depth      (depth) float64 0.0 0.2 0.4 0.6 0.8 ... 399.4 399.6 399.8 400.0\n",
      "  * ping_time  (ping_time) datetime64[ns] 2007-08-17T17:08:30 ... 2007-08-17T...\n",
      "Coordinates:\n",
      "  * channel    (channel) <U35 'GPT  18 kHz 009072034d55 3 ES18-11' ... 'GPT 2...\n",
      "  * depth      (depth) float64 0.0 0.2 0.4 0.6 0.8 ... 399.4 399.6 399.8 400.0\n",
      "  * ping_time  (ping_time) datetime64[ns] 2007-08-17T17:10:00 ... 2007-08-17T...\n",
      "Coordinates:\n",
      "  * depth      (depth) float64 0.0 0.2 0.4 0.6 0.8 ... 399.4 399.6 399.8 400.0\n",
      "  * ping_time  (ping_time) datetime64[ns] 2007-08-17T17:10:00 ... 2007-08-17T...\n",
      "Coordinates:\n",
      "  * channel    (channel) <U35 'GPT  18 kHz 009072034d55 3 ES18-11' ... 'GPT 2...\n",
      "  * depth      (depth) float64 0.0 0.2 0.4 0.6 0.8 ... 399.4 399.6 399.8 400.0\n",
      "  * ping_time  (ping_time) datetime64[ns] 2007-08-17T17:20:00 ... 2007-08-17T...\n",
      "Coordinates:\n",
      "  * depth      (depth) float64 0.0 0.2 0.4 0.6 0.8 ... 399.4 399.6 399.8 400.0\n",
      "  * ping_time  (ping_time) datetime64[ns] 2007-08-17T17:20:00 ... 2007-08-17T...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage dict\n",
    "data_mask_dir_dict = {'/home/exouser/hake_data/Sv_mask/hake_clean/2007': '/home/exouser/hake_data/Sv_regridded/2007',\n",
    "                    '/home/exouser/hake_data/Sv_mask/hake_clean/2009': '/home/exouser/hake_data/Sv_regridded/2009'}\n",
    "\n",
    "# Create an instance of HakeXarrayDatasets\n",
    "hake_dataset = HakeXarrayDatasets(data_mask_dir_dict, max_depth = 400)\n",
    "\n",
    "# Print the length of the dataset\n",
    "print(\"Dataset Length:\", len(hake_dataset))\n",
    "\n",
    "# Loop through the dataset and print some samples\n",
    "for idx in range(len(hake_dataset)):\n",
    "    data, target = hake_dataset[idx]\n",
    "\n",
    "    print(data.coords)\n",
    "    print(target.coords)\n",
    "\n",
    "    if idx == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 99.49it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 102.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Length: 765\n",
      "<xarray.Dataset>\n",
      "Dimensions:            (channel: 4, ping_time: 18, depth: 3795)\n",
      "Coordinates:\n",
      "  * channel            (channel) <U35 'GPT  18 kHz 009072034d55 3 ES18-11' .....\n",
      "  * depth              (depth) float64 0.0 0.2 0.4 0.6 ... 758.4 758.6 758.8\n",
      "  * ping_time          (ping_time) datetime64[ns] 2007-08-17T17:08:30 ... 200...\n",
      "Data variables:\n",
      "    Sv                 (channel, ping_time, depth) float64 ...\n",
      "    frequency_nominal  (channel) float64 ...\n",
      "    latitude           (ping_time) float64 ...\n",
      "    longitude          (ping_time) float64 ...\n",
      "Attributes:\n",
      "    processing_function:          commongrid.compute_MVBS\n",
      "    processing_level:             Level 3A\n",
      "    processing_level_url:         https://echopype.readthedocs.io/en/stable/p...\n",
      "    processing_software_name:     echopype\n",
      "    processing_software_version:  0.8.3.dev1+g87fd9af\n",
      "    processing_time:              2023-11-28T02:13:39Z\n",
      "<xarray.Dataset>\n",
      "Dimensions:            (channel: 4, ping_time: 120, depth: 3795)\n",
      "Coordinates:\n",
      "  * channel            (channel) <U35 'GPT  18 kHz 009072034d55 3 ES18-11' .....\n",
      "  * depth              (depth) float64 0.0 0.2 0.4 0.6 ... 758.4 758.6 758.8\n",
      "  * ping_time          (ping_time) datetime64[ns] 2007-08-17T17:10:00 ... 200...\n",
      "Data variables:\n",
      "    Sv                 (channel, ping_time, depth) float64 ...\n",
      "    frequency_nominal  (channel) float64 ...\n",
      "    latitude           (ping_time) float64 ...\n",
      "    longitude          (ping_time) float64 ...\n",
      "Attributes:\n",
      "    processing_function:          commongrid.compute_MVBS\n",
      "    processing_level:             Level 3A\n",
      "    processing_level_url:         https://echopype.readthedocs.io/en/stable/p...\n",
      "    processing_software_name:     echopype\n",
      "    processing_software_version:  0.8.3.dev1+g87fd9af\n",
      "    processing_time:              2023-11-28T02:13:39Z\n",
      "<xarray.Dataset>\n",
      "Dimensions:            (channel: 4, ping_time: 120, depth: 3795)\n",
      "Coordinates:\n",
      "  * channel            (channel) <U35 'GPT  18 kHz 009072034d55 3 ES18-11' .....\n",
      "  * depth              (depth) float64 0.0 0.2 0.4 0.6 ... 758.4 758.6 758.8\n",
      "  * ping_time          (ping_time) datetime64[ns] 2007-08-17T17:20:00 ... 200...\n",
      "Data variables:\n",
      "    Sv                 (channel, ping_time, depth) float64 ...\n",
      "    frequency_nominal  (channel) float64 ...\n",
      "    latitude           (ping_time) float64 ...\n",
      "    longitude          (ping_time) float64 ...\n",
      "Attributes:\n",
      "    processing_function:          commongrid.compute_MVBS\n",
      "    processing_level:             Level 3A\n",
      "    processing_level_url:         https://echopype.readthedocs.io/en/stable/p...\n",
      "    processing_software_name:     echopype\n",
      "    processing_software_version:  0.8.3.dev1+g87fd9af\n",
      "    processing_time:              2023-11-28T02:13:39Z\n"
     ]
    }
   ],
   "source": [
    "# Example usage ds_MVBS_dir_list\n",
    "ds_MVBS_dir_list =['/home/exouser/hake_data/Sv_regridded/2007', '/home/exouser/hake_data/Sv_regridded/2009']\n",
    "\n",
    "# Create an instance of HakeXarrayDatasets\n",
    "hake_dataset = HakeXarrayDatasets(ds_MVBS_dir_list)\n",
    "\n",
    "# Print the length of the dataset\n",
    "print(\"Dataset Length:\", len(hake_dataset))\n",
    "\n",
    "# Loop through the dataset and print some samples\n",
    "for idx in range(len(hake_dataset)):\n",
    "    data = hake_dataset[idx]\n",
    "\n",
    "    print(data)\n",
    "\n",
    "    if idx == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_MVBS = data\n",
    "\n",
    "# Partitioning and regridding\n",
    "resampled_obj = ds_MVBS[\"ping_time\"].resample(\n",
    "    ping_time=\"10min\", skipna=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing partitions: 100%|██████████| 24/24 [00:00<00:00, 57325.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2007-08-01T23:50:00.000000000 slice(0, 4, None)\n",
      "1\n",
      "2007-08-02T00:00:00.000000000 slice(4, 124, None)\n",
      "2\n",
      "2007-08-02T00:10:00.000000000 slice(124, 244, None)\n",
      "3\n",
      "2007-08-02T00:20:00.000000000 slice(244, 364, None)\n",
      "4\n",
      "2007-08-02T00:30:00.000000000 slice(364, 484, None)\n",
      "5\n",
      "2007-08-02T00:40:00.000000000 slice(484, 604, None)\n",
      "6\n",
      "2007-08-02T00:50:00.000000000 slice(604, 724, None)\n",
      "7\n",
      "2007-08-02T01:00:00.000000000 slice(724, 844, None)\n",
      "8\n",
      "2007-08-02T01:10:00.000000000 slice(844, 964, None)\n",
      "9\n",
      "2007-08-02T01:20:00.000000000 slice(964, 1084, None)\n",
      "10\n",
      "2007-08-02T01:30:00.000000000 slice(1084, 1204, None)\n",
      "11\n",
      "2007-08-02T01:40:00.000000000 slice(1204, 1324, None)\n",
      "12\n",
      "2007-08-02T01:50:00.000000000 slice(1324, 1444, None)\n",
      "13\n",
      "2007-08-02T02:00:00.000000000 slice(1444, 1564, None)\n",
      "14\n",
      "2007-08-02T02:10:00.000000000 slice(1564, 1684, None)\n",
      "15\n",
      "2007-08-02T02:20:00.000000000 slice(1684, 1804, None)\n",
      "16\n",
      "2007-08-02T02:30:00.000000000 slice(1804, 1924, None)\n",
      "17\n",
      "2007-08-02T02:40:00.000000000 slice(1924, 2044, None)\n",
      "18\n",
      "2007-08-02T02:50:00.000000000 slice(2044, 2164, None)\n",
      "19\n",
      "2007-08-02T03:00:00.000000000 slice(2164, 2284, None)\n",
      "20\n",
      "2007-08-02T03:10:00.000000000 slice(2284, 2404, None)\n",
      "21\n",
      "2007-08-02T03:20:00.000000000 slice(2404, 2524, None)\n",
      "22\n",
      "2007-08-02T03:30:00.000000000 slice(2524, 2644, None)\n",
      "23\n",
      "2007-08-02T03:40:00.000000000 slice(2644, None, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for pt_index, (k, v) in enumerate(\n",
    "    tqdm(resampled_obj.groups.items(), desc=\"Processing partitions\")\n",
    "):\n",
    "    print(pt_index)\n",
    "    print(k, v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hake_ml_poc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
