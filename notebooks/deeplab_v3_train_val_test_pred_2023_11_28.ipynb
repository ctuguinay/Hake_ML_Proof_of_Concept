{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import xarray as xr\n",
    "from torch.utils.data import random_split\n",
    "import pytorch_lightning as pl\n",
    "from typing import Dict, List, Union\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision.models.segmentation.deeplabv3 import ASPP\n",
    "from torchvision.models.resnet import resnet50\n",
    "from torchvision.models._utils import IntermediateLayerGetter\n",
    "from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HakeXarrayDatasets(Dataset):\n",
    "    def __init__(self,\n",
    "                 data: Union[List[str],Dict[str, str]],\n",
    "                 desired_order: List[str] = [\"120 kHz\", \"38 kHz\", \"18 kHz\"],\n",
    "                 slice_length: int=1000,\n",
    "                 overlap: int=500):\n",
    "        self.desired_order = desired_order\n",
    "        self.slice_length = slice_length\n",
    "        self.overlap = overlap\n",
    "\n",
    "        if isinstance(data, list):\n",
    "            self.is_predict = True\n",
    "            self.file_dict = {}\n",
    "            self.load_from_data_dir_list(data)\n",
    "        elif isinstance(data, dict):\n",
    "            self.is_predict = False\n",
    "            self.file_dict = {}\n",
    "            self.load_from_target_data_dict(data)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid input type for 'data'. It should be either a list or a dictionary.\")\n",
    "        \n",
    "    def generate_overlapping_slices(self, ds):\n",
    "        ds_slices = [slice(len(ds[\"ping_time\"]) - self.slice_length, len(ds[\"ping_time\"]))]\n",
    "        start_idx = 0\n",
    "        while start_idx + self.slice_length <= len(ds[\"ping_time\"]):\n",
    "            end_idx = start_idx + self.slice_length\n",
    "            ds_slices.append(slice(start_idx, end_idx))\n",
    "            start_idx += self.slice_length - self.overlap\n",
    "\n",
    "        return ds_slices\n",
    "\n",
    "    def load_from_target_data_dict(self, target_data_dir_dict):\n",
    "        counter = 0\n",
    "        for mask_dir, ds_MVBS_dir in target_data_dir_dict.items():\n",
    "            ds_MVBS_dir = Path(ds_MVBS_dir)\n",
    "            mask_dir = Path(mask_dir)\n",
    "            ds_MVBS_files = list(ds_MVBS_dir.glob('*.zarr'))\n",
    "            for ds_MVBS_file in tqdm(ds_MVBS_files):\n",
    "                ds_MVBS_filename = ds_MVBS_file.name\n",
    "                mask_file = self.find_matching_target(ds_MVBS_filename, mask_dir)\n",
    "                if mask_file is not None:\n",
    "                    if self.check_ds_MVBS_file(ds_MVBS_file):\n",
    "                        ds_MVBS = xr.open_dataset(ds_MVBS_file)\n",
    "                        ds_slices = self.generate_overlapping_slices(ds_MVBS)\n",
    "                        for ds_slice in ds_slices:\n",
    "                            self.file_dict[counter] = {\"ds_MVBS_file\": str(ds_MVBS_file),\n",
    "                                                    \"mask_file\": str(mask_file),\n",
    "                                                    \"ping_time_slice\": ds_slice}\n",
    "                            counter += 1\n",
    "\n",
    "    def load_from_data_dir_list(self, ds_MVBS_dirs):\n",
    "        counter = 0\n",
    "        for ds_MVBS_dir in ds_MVBS_dirs:\n",
    "            ds_MVBS_dir = Path(ds_MVBS_dir)\n",
    "            unchecked_ds_MVBS_files = list(ds_MVBS_dir.glob('*.zarr'))\n",
    "            for ds_MVBS_file in tqdm(unchecked_ds_MVBS_files):\n",
    "                if self.check_ds_MVBS_file(ds_MVBS_file):\n",
    "                    ds_MVBS = xr.open_dataset(ds_MVBS_file)\n",
    "                    ds_slices = self.generate_overlapping_slices(ds_MVBS)\n",
    "                    for ds_slice in ds_slices:\n",
    "                        self.file_dict[counter] = {\"ds_MVBS_file\": str(ds_MVBS_file),\n",
    "                                                   \"ping_time_slice\": ds_slice}\n",
    "                        counter += 1\n",
    "\n",
    "    def check_ds_MVBS_file(self, ds_MVBS_file):\n",
    "        try:\n",
    "            data_ds = xr.open_dataset(ds_MVBS_file)\n",
    "            data_channels = data_ds[\"Sv\"].channel.values\n",
    "            return all(any(partial_name in ch for ch in data_channels) for partial_name in self.desired_order)\n",
    "        except KeyError as e:\n",
    "            print(f\"KeyError: {e}, File Paths: {ds_MVBS_file}\")\n",
    "            return False\n",
    "\n",
    "    def find_matching_target(self, ds_MVBS_filename, mask_dir):\n",
    "        mask_files = list(mask_dir.glob('*.zarr'))\n",
    "        for mask_file in mask_files:\n",
    "            if mask_file.name == ds_MVBS_filename:\n",
    "                return mask_file\n",
    "        return None\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.file_dict:\n",
    "            return len(self.file_dict)\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.file_dict is not None and self.is_predict:\n",
    "            file_dict_idx = self.file_dict[idx]\n",
    "\n",
    "            da_MVBS_tensor = torch.tensor(xr.open_dataset(\n",
    "                file_dict_idx[\"ds_MVBS_file\"]\n",
    "            )[\"Sv\"].isel(ping_time=file_dict_idx[\"ping_time_slice\"]\n",
    "            ).transpose(\"channel\", \"depth\", \"ping_time\").data, dtype=torch.float32)\n",
    "\n",
    "            return da_MVBS_tensor, file_dict_idx\n",
    "\n",
    "        elif self.file_dict is not None and not self.is_predict:\n",
    "            file_dict_idx = self.file_dict[idx]\n",
    "\n",
    "            da_MVBS_tensor = torch.tensor(xr.open_dataset(\n",
    "                file_dict_idx[\"ds_MVBS_file\"]\n",
    "            )[\"Sv\"].isel(ping_time=file_dict_idx[\"ping_time_slice\"]\n",
    "            ).transpose(\"channel\", \"depth\", \"ping_time\").data, dtype=torch.float32)\n",
    "\n",
    "            mask_tensor = torch.tensor(xr.open_dataset(\n",
    "                file_dict_idx[\"mask_file\"]\n",
    "            )[\"mask\"].isel(ping_time=file_dict_idx[\"ping_time_slice\"]\n",
    "            ).transpose(\"depth\", \"ping_time\").data, dtype=torch.float32)\n",
    "\n",
    "            print(xr.open_dataset(\n",
    "                file_dict_idx[\"mask_file\"]\n",
    "            )[\"mask\"].isel(ping_time=file_dict_idx[\"ping_time_slice\"]\n",
    "            )[\"ping_time\"])\n",
    "\n",
    "            return da_MVBS_tensor, mask_tensor, file_dict_idx\n",
    "        else:\n",
    "            raise IndexError(\"Index out of range. No ds_MVBSfiles available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214/214 [00:01<00:00, 186.75it/s]\n",
      "100%|██████████| 166/166 [00:00<00:00, 284.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Length: 536\n",
      "<xarray.DataArray 'ping_time' (ping_time: 1000)>\n",
      "array(['2007-08-17T18:29:05.000000000', '2007-08-17T18:29:10.000000000',\n",
      "       '2007-08-17T18:29:15.000000000', ..., '2007-08-17T19:52:10.000000000',\n",
      "       '2007-08-17T19:52:15.000000000', '2007-08-17T19:52:20.000000000'],\n",
      "      dtype='datetime64[ns]')\n",
      "Coordinates:\n",
      "  * ping_time  (ping_time) datetime64[ns] 2007-08-17T18:29:05 ... 2007-08-17T...\n",
      "Attributes:\n",
      "    axis:           T\n",
      "    long_name:      Ping time\n",
      "    standard_name:  time\n",
      "Data Shape: torch.Size([4, 3795, 1000])\n",
      "Target Shape: torch.Size([3795, 1000])\n",
      "{'ds_MVBS_file': '/home/exouser/hake_data/Sv_regridded/2007/x0116_0_wt_20070817_170834_f0006.zarr', 'mask_file': '/home/exouser/hake_data/Sv_mask/hake_clean/2007/x0116_0_wt_20070817_170834_f0006.zarr', 'ping_time_slice': slice(967, 1967, None)}\n",
      "<xarray.DataArray 'ping_time' (ping_time: 1000)>\n",
      "array(['2007-08-17T17:08:30.000000000', '2007-08-17T17:08:35.000000000',\n",
      "       '2007-08-17T17:08:40.000000000', ..., '2007-08-17T18:31:35.000000000',\n",
      "       '2007-08-17T18:31:40.000000000', '2007-08-17T18:31:45.000000000'],\n",
      "      dtype='datetime64[ns]')\n",
      "Coordinates:\n",
      "  * ping_time  (ping_time) datetime64[ns] 2007-08-17T17:08:30 ... 2007-08-17T...\n",
      "Attributes:\n",
      "    axis:           T\n",
      "    long_name:      Ping time\n",
      "    standard_name:  time\n",
      "Data Shape: torch.Size([4, 3795, 1000])\n",
      "Target Shape: torch.Size([3795, 1000])\n",
      "{'ds_MVBS_file': '/home/exouser/hake_data/Sv_regridded/2007/x0116_0_wt_20070817_170834_f0006.zarr', 'mask_file': '/home/exouser/hake_data/Sv_mask/hake_clean/2007/x0116_0_wt_20070817_170834_f0006.zarr', 'ping_time_slice': slice(0, 1000, None)}\n",
      "<xarray.DataArray 'ping_time' (ping_time: 1000)>\n",
      "array(['2007-08-17T17:50:10.000000000', '2007-08-17T17:50:15.000000000',\n",
      "       '2007-08-17T17:50:20.000000000', ..., '2007-08-17T19:13:15.000000000',\n",
      "       '2007-08-17T19:13:20.000000000', '2007-08-17T19:13:25.000000000'],\n",
      "      dtype='datetime64[ns]')\n",
      "Coordinates:\n",
      "  * ping_time  (ping_time) datetime64[ns] 2007-08-17T17:50:10 ... 2007-08-17T...\n",
      "Attributes:\n",
      "    axis:           T\n",
      "    long_name:      Ping time\n",
      "    standard_name:  time\n",
      "Data Shape: torch.Size([4, 3795, 1000])\n",
      "Target Shape: torch.Size([3795, 1000])\n",
      "{'ds_MVBS_file': '/home/exouser/hake_data/Sv_regridded/2007/x0116_0_wt_20070817_170834_f0006.zarr', 'mask_file': '/home/exouser/hake_data/Sv_mask/hake_clean/2007/x0116_0_wt_20070817_170834_f0006.zarr', 'ping_time_slice': slice(500, 1500, None)}\n"
     ]
    }
   ],
   "source": [
    "# Example usage dict\n",
    "data_mask_dir_dict = {'/home/exouser/hake_data/Sv_mask/hake_clean/2007': '/home/exouser/hake_data/Sv_regridded/2007',\n",
    "                    '/home/exouser/hake_data/Sv_mask/hake_clean/2009': '/home/exouser/hake_data/Sv_regridded/2009'}\n",
    "\n",
    "# Create an instance of HakeXarrayDatasets\n",
    "hake_dataset = HakeXarrayDatasets(data_mask_dir_dict)\n",
    "\n",
    "# Print the length of the dataset\n",
    "print(\"Dataset Length:\", len(hake_dataset))\n",
    "\n",
    "data_dim = None\n",
    "target_dim = None\n",
    "\n",
    "# Loop through the dataset and print some samples\n",
    "for idx in range(len(hake_dataset)):\n",
    "    data, target, file_dict = hake_dataset[idx]\n",
    "\n",
    "    print(\"Data Shape:\", data.shape)\n",
    "    print(\"Target Shape:\", target.shape)\n",
    "    print(file_dict)\n",
    "\n",
    "    if idx == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214/214 [00:02<00:00, 90.30it/s]\n",
      "100%|██████████| 166/166 [00:02<00:00, 78.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Length: 3321\n",
      "Data Shape: torch.Size([4, 3795, 240])\n",
      "{'ds_MVBS_file': '/home/exouser/hake_data/Sv_regridded/2007/x0116_0_wt_20070817_170834_f0006.zarr', 'ping_time_slice': slice(1727, 1967, None)}\n",
      "Data Shape: torch.Size([4, 3795, 240])\n",
      "{'ds_MVBS_file': '/home/exouser/hake_data/Sv_regridded/2007/x0116_0_wt_20070817_170834_f0006.zarr', 'ping_time_slice': slice(0, 240, None)}\n",
      "Data Shape: torch.Size([4, 3795, 240])\n",
      "{'ds_MVBS_file': '/home/exouser/hake_data/Sv_regridded/2007/x0116_0_wt_20070817_170834_f0006.zarr', 'ping_time_slice': slice(160, 400, None)}\n"
     ]
    }
   ],
   "source": [
    "# Example usage ds_MVBS_dir_list\n",
    "ds_MVBS_dir_list =['/home/exouser/hake_data/Sv_regridded/2007', '/home/exouser/hake_data/Sv_regridded/2009']\n",
    "\n",
    "# Create an instance of HakeXarrayDatasets\n",
    "hake_dataset = HakeXarrayDatasets(ds_MVBS_dir_list)\n",
    "\n",
    "# Print the length of the dataset\n",
    "print(\"Dataset Length:\", len(hake_dataset))\n",
    "\n",
    "# Loop through the dataset and print some samples\n",
    "for idx in range(len(hake_dataset)):\n",
    "    data, file_dict = hake_dataset[idx]\n",
    "\n",
    "    print(\"Data Shape:\", data.shape)\n",
    "    print(file_dict)\n",
    "\n",
    "    if idx == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HakeDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, training_target_data_dir_dict: dict, testing_target_data_dir_dict: dict,\n",
    "                 pred_dir: dict, batch_size=32, num_workers=4, validation_split=0.1):\n",
    "        super(HakeDataModule, self).__init__()\n",
    "        self.training_target_data_dir_dict = training_target_data_dir_dict\n",
    "        self.testing_target_data_dir_dict = testing_target_data_dir_dict\n",
    "        self.pred_dir = pred_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.validation_split = validation_split\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == 'fit' or stage is None:\n",
    "            full_train_dataset = HakeXarrayDatasets(self.training_target_data_dir_dict)\n",
    "            train_size = int((1 - self.validation_split) * len(full_train_dataset))\n",
    "            val_size = len(full_train_dataset) - train_size\n",
    "            self.train_dataset, self.val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        test_dataset = HakeXarrayDatasets(self.testing_target_data_dir_dict)\n",
    "        return DataLoader(test_dataset, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        pred_dataset = HakeXarrayDatasets(self.pred_dir)\n",
    "        return DataLoader(pred_dataset, batch_size=self.batch_size, num_workers=self.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214/214 [00:01<00:00, 187.38it/s]\n",
      "100%|██████████| 166/166 [00:00<00:00, 286.49it/s]\n"
     ]
    }
   ],
   "source": [
    "training_target_data_dir_dict = {'/home/exouser/hake_data/Sv_mask/hake_clean/2007': '/home/exouser/hake_data/Sv_regridded/2007',\n",
    "                    '/home/exouser/hake_data/Sv_mask/hake_clean/2009': '/home/exouser/hake_data/Sv_regridded/2009'}\n",
    "testing_target_data_dir_dict = {'/home/exouser/hake_data/Sv_mask/hake_clean/20011': '/home/exouser/hake_data/Sv_regridded/20011',\n",
    "                    '/home/exouser/hake_data/Sv_mask/hake_clean/2013': '/home/exouser/hake_data/Sv_regridded/2013'}\n",
    "pred_dir = ['/home/exouser/hake_data/Sv_regridded/2017', '/home/exouser/hake_data/Sv_regridded/2019']\n",
    "\n",
    "data_module = HakeDataModule(\n",
    "    training_target_data_dir_dict=training_target_data_dir_dict,\n",
    "    testing_target_data_dir_dict=testing_target_data_dir_dict,\n",
    "    pred_dir=pred_dir,\n",
    "    batch_size=32,\n",
    "    num_workers=4,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 176/176 [00:01<00:00, 102.64it/s]\n",
      "100%|██████████| 167/167 [00:01<00:00, 125.68it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/exouser/miniforge3/envs/hake_ml_poc/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 127, in collate\n    return elem_type({key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem})\n  File \"/home/exouser/miniforge3/envs/hake_ml_poc/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 127, in <dictcomp>\n    return elem_type({key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem})\n  File \"/home/exouser/miniforge3/envs/hake_ml_poc/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 150, in collate\n    raise TypeError(default_collate_err_msg_format.format(elem_type))\nTypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'slice'>\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/exouser/miniforge3/envs/hake_ml_poc/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/exouser/miniforge3/envs/hake_ml_poc/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/home/exouser/miniforge3/envs/hake_ml_poc/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 265, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/home/exouser/miniforge3/envs/hake_ml_poc/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 142, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/exouser/miniforge3/envs/hake_ml_poc/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 142, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/exouser/miniforge3/envs/hake_ml_poc/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 130, in collate\n    return {key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem}\n  File \"/home/exouser/miniforge3/envs/hake_ml_poc/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 130, in <dictcomp>\n    return {key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem}\n  File \"/home/exouser/miniforge3/envs/hake_ml_poc/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 150, in collate\n    raise TypeError(default_collate_err_msg_format.format(elem_type))\nTypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'slice'>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/exouser/Hake_ML_Proof_of_Concept/notebooks/deeplab_v3_train_val_test_pred_2023_11_28.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B149.165.171.159/home/exouser/Hake_ML_Proof_of_Concept/notebooks/deeplab_v3_train_val_test_pred_2023_11_28.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m predict_dataloader \u001b[39m=\u001b[39m data_module\u001b[39m.\u001b[39mpredict_dataloader()\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B149.165.171.159/home/exouser/Hake_ML_Proof_of_Concept/notebooks/deeplab_v3_train_val_test_pred_2023_11_28.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m predict_dataloader:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B149.165.171.159/home/exouser/Hake_ML_Proof_of_Concept/notebooks/deeplab_v3_train_val_test_pred_2023_11_28.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(batch)\n",
      "File \u001b[0;32m~/miniforge3/envs/hake_ml_poc/lib/python3.9/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniforge3/envs/hake_ml_poc/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1345\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1345\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[0;32m~/miniforge3/envs/hake_ml_poc/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   1370\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1371\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   1372\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniforge3/envs/hake_ml_poc/lib/python3.9/site-packages/torch/_utils.py:694\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    691\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    692\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    693\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 694\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/exouser/miniforge3/envs/hake_ml_poc/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 127, in collate\n    return elem_type({key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem})\n  File \"/home/exouser/miniforge3/envs/hake_ml_poc/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 127, in <dictcomp>\n    return elem_type({key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem})\n  File \"/home/exouser/miniforge3/envs/hake_ml_poc/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 150, in collate\n    raise TypeError(default_collate_err_msg_format.format(elem_type))\nTypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'slice'>\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/exouser/miniforge3/envs/hake_ml_poc/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/exouser/miniforge3/envs/hake_ml_poc/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/home/exouser/miniforge3/envs/hake_ml_poc/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 265, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/home/exouser/miniforge3/envs/hake_ml_poc/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 142, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/exouser/miniforge3/envs/hake_ml_poc/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 142, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/exouser/miniforge3/envs/hake_ml_poc/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 130, in collate\n    return {key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem}\n  File \"/home/exouser/miniforge3/envs/hake_ml_poc/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 130, in <dictcomp>\n    return {key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem}\n  File \"/home/exouser/miniforge3/envs/hake_ml_poc/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 150, in collate\n    raise TypeError(default_collate_err_msg_format.format(elem_type))\nTypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'slice'>\n"
     ]
    }
   ],
   "source": [
    "predict_dataloader = data_module.predict_dataloader()\n",
    "for batch in predict_dataloader:\n",
    "    print(batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hake_ml_poc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
